# Llamaindex

## åŸºç¡€ä»»åŠ¡

### ä»»åŠ¡è¦æ±‚1ï¼šåŸºäº LlamaIndex æ„å»ºè‡ªå·±çš„ RAG çŸ¥è¯†åº“ï¼Œå¯»æ‰¾ä¸€ä¸ªé—®é¢˜ A åœ¨ä½¿ç”¨ LlamaIndex ä¹‹å‰ æµ¦è¯­ API ä¸ä¼šå›ç­”ï¼Œå€ŸåŠ© LlamaIndex å æµ¦è¯­ API å…·å¤‡å›ç­” A çš„èƒ½åŠ›ï¼Œæˆªå›¾ä¿å­˜ã€‚

æ•™ç¨‹ç¬¬ä¸€æ­¥å°±æ˜¯æå‡é…ç½®ï¼Œä¸è¿‡äººå®¶æ˜¯ç›´æ¥å¼€çš„æ–°å¼€å‘æœºï¼Œæˆ‘è¿™å„¿é€‰æ‹©æå‡é…ç½®â€¦â€¦

![1732966452893](image/readme/1732966452893.png)

ç„¶åæ•™ç¨‹æ˜¯ç»™çš„åˆ›å»ºä¸€ä¸ªæ–°çš„è™šæ‹Ÿpythonç¯å¢ƒï¼Œä¿ºä¹Ÿä¾è‘«èŠ¦ç”»ç“¢æä¸€ä¸ªã€‚(æˆ‘é€‰æ‹©åœ¨webä¸­ç”¨ç»ˆç«¯â€¦â€¦VScodeçš„sshè¾“å…¥å‘½ä»¤ä¸çŸ¥é“ä¸ºä»€ä¹ˆç‰¹åˆ«å¡â€¦â€¦)

![1733139476937](image/readme/1733139476937.png)

![1733139801825](image/readme/1733139801825.png)

æ¥ä¸‹æ¥æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼Œå¹¶ä¸”å®‰è£…ç›¸å…³ä¾èµ–å’Œåé¢è¦ç”¨åˆ°çš„åº“ã€‚

```
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
conda activate llamaindex

# å®‰è£…ä¾èµ–å’Œåº“
pip install einops==0.7.0 protobuf==5.26.1
pip install llama-index==0.11.20
pip install llama-index-llms-replicate==0.3.0
pip install llama-index-llms-openai-like==0.2.0
pip install llama-index-embeddings-huggingface==0.3.1
pip install llama-index-embeddings-instructor==0.2.1
pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu121
```

éšåè¿›å…¥æ ¹ç›®å½•ï¼Œå¼€ä¸ªæ–‡ä»¶å¤¹ï¼Œæ”¾æˆ‘ä»¬çš„æ¨¡å‹

![1733142384939](image/readme/1733142384939.png)

ç„¶åå’Œæ•™ç¨‹çš„ä¸€æ ·ï¼Œè¿›å…¥ç›®å½•ï¼Œè®¾ç½®å¥½ä¸‹è½½çš„è„šæœ¬

![1733142508228](image/readme/1733142508228.png)

åœ¨æ¿€æ´»äº†è™šæ‹Ÿç¯å¢ƒçš„æƒ…å†µä¸‹ï¼Œç›´æ¥è¿è¡Œï¼Œå¼€å§‹ä¸‹è½½

![1733142557415](image/readme/1733142557415.png)

![1733142585258](image/readme/1733142585258.png)

ç¨å¾®è€å¿ƒç­‰å¾…ä¸€ä¸‹ã€‚

å¥½äº†ä¹‹åæŒ‰ç…§æ•™ç¨‹ï¼Œå’±ä»¬ä¸‹ä¸€ä¸‹æ¨¡å‹ã€‚

```
# ä¸ç”¨è¿™ä¸ªçš„è¯å¯èƒ½ä¹‹å‰æ²¡æœ‰å®‰è£…è¿‡è¿™ä¸ªï¼Œå¯¼è‡´åé¢çš„å‘½ä»¤ä¸è®¤â€¦â€¦
apt install git-lfs 

git lfs install

cd /root/model/

git clone https://www.modelscope.cn/Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2.git

mv paraphrase-multilingual-MiniLM-L12-v2 sentence-transformer
```

å¯ä»¥çœ‹åˆ°å›¾ç‰‡é‡Œè¿™æ ·ï¼Œå°±æ˜¯æŠŠè¦çš„ä¸œè¥¿ä¸‹è½½å¥½äº†â€¦â€¦

![1733143651013](image/readme/1733143651013.png)

ä¸ºäº†é¿å…ç½‘ç»œé—®é¢˜ï¼Œè¿™é‡Œå†ä¸‹è½½ä¸€ä¸‹NLTKçš„ç›¸å…³èµ„æºã€‚

```
cd /root
git clone https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages
cd nltk_data
mv packages/*  ./
cd tokenizers
unzip punkt.zip
cd ../taggers
unzip averaged_perceptron_tagger.zip
```

æ¥ä¸‹æ¥å°±æ˜¯å¯¹æ¯”äº†

### ä¸ä½¿ç”¨ LlamaIndex RAGï¼ˆä»…APIï¼‰

å…ˆæ˜¯å¼€ä¸€ä¸ªpyæ–‡ä»¶ã€‚

```
cd ~/llamaindex_demo

touch test_internlm.py
```

![1733144056685](image/readme/1733144056685.png)

æˆ‘è¿™é‡Œæ˜¯ç›´æ¥å¤åˆ¶çš„â€¦â€¦åé¢è¿˜éœ€è¦å°å°çš„ä¿®æ”¹ä¸€ä¸‹ä¸‹ï¼Œæ¯”å¦‚è¯´tokenå¾—ç”¨è‡ªå·±çš„æ~

è¿è¡Œåç»“æœå¦‚å›¾ï¼š

![1733144239483](image/readme/1733144239483.png)

é‚£ä¹ˆæ¥ä¸‹æ¥å¾ˆç®€å•ï¼Œå°±æ˜¯åŠ ä¸ŠLlamaIndexåå†é—®ä¸€ä¸‹è¿™ä¸ªé—®é¢˜ã€‚

æˆ‘ä»¬å‰ç½®å·¥ä½œå·²ç»åšå¥½ï¼Œæ¥ä¸‹æ¥å°±æ˜¯åŠ ä¸€ä¸‹è¿™ä¸ªçŸ¥è¯†ï¼ˆxtuneræ˜¯å•¥ï¼Œå…¶å®ä¹Ÿå¯ä»¥ç”¨åˆ«çš„ï¼‰

```
cd ~/llamaindex_demo
mkdir data
cd data
git clone https://github.com/InternLM/xtuner.git
mv xtuner/README_zh-CN.md ./
```

![1733144537646](image/readme/1733144537646.png)

æˆ‘ä»¬è¿™ä¸ªå°±æ˜¯ç›´æ¥ä¸‹è½½äº†xtunerï¼Œç„¶åå†ç§»åŠ¨äº†ä¸€ä¸‹å®ƒçš„ä»‹ç»ã€‚

    å›åˆ°ä¸€æ ·çš„ä½ç½®ï¼Œå†åŠ ä¸€ä¸ªpyè„šæœ¬ã€‚

```
cd ~/llamaindex_demo

touch llamaindex_RAG.py
```

pyçš„å†…å®¹å¦‚ä¸‹

```
import os 
os.environ['NLTK_DATA'] = '/root/nltk_data'

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.settings import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.legacy.callbacks import CallbackManager
from llama_index.llms.openai_like import OpenAILike


# Create an instance of CallbackManager
callback_manager = CallbackManager()

api_base_url =  "https://internlm-chat.intern-ai.org.cn/puyu/api/v1/"
model = "internlm2.5-latest"
api_key = "è¯·å¡«å†™ API Key"

# api_base_url =  "https://api.siliconflow.cn/v1"
# model = "internlm/internlm2_5-7b-chat"
# api_key = "è¯·å¡«å†™ API Key"



llm =OpenAILike(model=model, api_base=api_base_url, api_key=api_key, is_chat_model=True,callback_manager=callback_manager)


#åˆå§‹åŒ–ä¸€ä¸ªHuggingFaceEmbeddingå¯¹è±¡ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
embed_model = HuggingFaceEmbedding(
#æŒ‡å®šäº†ä¸€ä¸ªé¢„è®­ç»ƒçš„sentence-transformeræ¨¡å‹çš„è·¯å¾„
    model_name="/root/model/sentence-transformer"
)
#å°†åˆ›å»ºçš„åµŒå…¥æ¨¡å‹èµ‹å€¼ç»™å…¨å±€è®¾ç½®çš„embed_modelå±æ€§ï¼Œ
#è¿™æ ·åœ¨åç»­çš„ç´¢å¼•æ„å»ºè¿‡ç¨‹ä¸­å°±ä¼šä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚
Settings.embed_model = embed_model

#åˆå§‹åŒ–llm
Settings.llm = llm

#ä»æŒ‡å®šç›®å½•è¯»å–æ‰€æœ‰æ–‡æ¡£ï¼Œå¹¶åŠ è½½æ•°æ®åˆ°å†…å­˜ä¸­
documents = SimpleDirectoryReader("/root/llamaindex_demo/data").load_data()
#åˆ›å»ºä¸€ä¸ªVectorStoreIndexï¼Œå¹¶ä½¿ç”¨ä¹‹å‰åŠ è½½çš„æ–‡æ¡£æ¥æ„å»ºç´¢å¼•ã€‚
# æ­¤ç´¢å¼•å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶å­˜å‚¨è¿™äº›å‘é‡ä»¥ä¾¿äºå¿«é€Ÿæ£€ç´¢ã€‚
index = VectorStoreIndex.from_documents(documents)
# åˆ›å»ºä¸€ä¸ªæŸ¥è¯¢å¼•æ“ï¼Œè¿™ä¸ªå¼•æ“å¯ä»¥æ¥æ”¶æŸ¥è¯¢å¹¶è¿”å›ç›¸å…³æ–‡æ¡£çš„å“åº”ã€‚
query_engine = index.as_query_engine()
response = query_engine.query("xtuneræ˜¯ä»€ä¹ˆ?")

print(response)
```

![1733144739390](image/readme/1733144739390.png)

ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼šï¼ˆä¸å¾—ä¸è¯´ç¡®å®ç­‰äº†ä¸€ç‚¹æ—¶é—´â€¦â€¦ï¼‰

![1733145101186](image/readme/1733145101186.png)

### ä»»åŠ¡è¦æ±‚2ï¼ˆå¯é€‰ï¼Œå‚è€ƒreadme.mdï¼‰ï¼šåŸºäº LlamaIndex æ„å»ºè‡ªå·±çš„ RAG çŸ¥è¯†åº“ï¼Œå¯»æ‰¾ä¸€ä¸ªé—®é¢˜ A åœ¨ä½¿ç”¨ LlamaIndex ä¹‹å‰ InternLM2-Chat-1.8B æ¨¡å‹ä¸ä¼šå›ç­”ï¼Œå€ŸåŠ© LlamaIndex å InternLM2-Chat-1.8B æ¨¡å‹å…·å¤‡å›ç­” A çš„èƒ½åŠ›ï¼Œæˆªå›¾ä¿å­˜ã€‚

æœ¬è´¨ä¸Šå’Œæˆ‘ä»¬ä¹‹å‰åšçš„ä»»åŠ¡ä¸€æ²¡æœ‰åŒºåˆ«â€¦â€¦ç­‰æˆ‘ç©ºäº†è¡¥ä¸Šâ€¦â€¦

### ä»»åŠ¡è¦æ±‚3ï¼ˆä¼˜ç§€å­¦å‘˜å¿…åšï¼‰ï¼šå°† Streamlit+LlamaIndex+æµ¦è¯­APIçš„ Space éƒ¨ç½²åˆ° Hugging Faceã€‚

#### HFç¤¾åŒºä¸­çš„Spaceåˆ›å»º

è¿™é‡Œé¢å…¶å®ä¸æ˜¯å¾ˆéš¾ï¼Œæˆ‘ä»¬å…ˆç™»å½•HFä¸Šå»ï¼Œè¿™ç©æ„å„¿ä¸€å¼€å§‹æˆ‘æ­»æ´»ä¸Šä¸å»â€¦â€¦

æ³¨å†Œä»€ä¹ˆçš„æˆ‘å°±ä¸è¯´äº†ï¼Œåæ­£å¾ˆç®€å•çš„ã€‚è¿›å»ä»¥åç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬å¼€ä¸€ä¸ªæ–°çš„ `Space`

```
https://huggingface.co/spaces
```

åœ¨è¿™é‡Œé¢çš„å³ä¸ŠåŒºåŸŸæœ‰ä¸€ä¸ªåˆ›å»ºæ–°çš„ç©ºé—´ï¼ˆæˆ‘ç›´è¯‘äº†ï¼‰

![1733199397871](image/readme/1733199397871.png)

ç„¶åç®€å•å¡«ä¸€ä¸‹ä¸‹é¢çš„ä¿¡æ¯ï¼Œè®°å¾—é€‰æ‹©ä¸‹é¢é‚£ä¸ª `streamlit`ï¼Œè¿™ä¸ªæ˜¯æˆ‘ä»¬è¿™ä¸ªè¯¾çš„ä»»åŠ¡æã€‚

![1733199431110](image/readme/1733199431110.png)

é€‰å¥½ä»¥åä¼šæœ‰ä¸€ä¸ªgitçš„é“¾æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨åé¢çš„codespaceä¸­è¾“å…¥å®ƒï¼Œå°†æˆ‘ä»¬è¿™ä¸ªåˆ›å»ºçš„ç©ºé—´å…‹éš†åˆ°codespaceä¸­ï¼Œè¿™é‡Œå› ä¸ºæˆ‘çš„å¤±è¯¯ï¼Œæ²¡æœ‰æˆªå›¾ä¿ç•™ï¼Œæ‰€ä»¥æ²¡åŠæ³•æš‚æ—¶äº†ï¼Œåªå¥½æ‹¿ä¸€ä¸ªåé¢çš„å›¾ç‰‡é¡¶ä¸€ä¸‹ã€‚ï¼ˆé“¾æ¥å°±æ˜¯httpsé‚£ä¸€ä¸²ï¼‰

å…¶å®ä¹Ÿæ˜¯å¾ˆå¥½æ‡‚çš„ï¼Œæœ€åé¢çš„å°±æ˜¯æˆ‘ä»¬ä¹‹å‰è®¾ç½®çš„ç©ºé—´åå­—ï¼Œå¦å¤–spacesåé¢çš„é‚£ä¸ªå°±æ˜¯æˆ‘ä»¬è‡ªå·±çš„ç”¨æˆ·åå­—äº†â€¦â€¦æˆ‘å°±ä¸æ¼å‡ºæ¥äº†ã€‚

![1733199588874](image/readme/1733199588874.png)

è¿™è¾¹åŸºæœ¬æš‚æ—¶ç”¨ä¸ä¸Šäº†ï¼Œæˆ‘ä»¬ä¸‹é¢è½¬åˆ°codespaceä¸­å»ã€‚

#### codespaceçš„åˆ›å»ºä»¥åŠå†…éƒ¨é…ç½®

é¦–å…ˆè¿›å…¥codespaceè¿™ä¸ªç½‘å€

```
https://github.com/codespaces
```

![1733199917200](image/readme/1733199917200.png)

è¿™è¾¹éšä¾¿é€‰ä¸€ä¸ªéƒ½æ˜¯å¯ä»¥çš„ï¼Œåæ­£æˆ‘ä»¬è¿™è¾¹çš„codespaceåªæ˜¯ä¸€ä¸ªä»“åº“çš„æ€§è´¨ã€‚

è¿›å…¥åæˆ‘ä»¬æŠŠä¹‹å‰çš„é‚£ä¸ªHFçš„æ–°Spaceå…ˆcloneä¸‹æ¥ã€‚

![1733200011200](image/readme/1733200011200.png)



æ¥ä¸‹æ¥ç›´æ¥å°±æ˜¯è¿›å…¥çš„æ–‡ä»¶å¤¹é‡Œé¢ç„¶åè·å–ä¸€ä¸‹çŸ¥è¯†åº“ï¼Œé…ç½®å¥½å°±å¯ä»¥äº†


```
mkdir data
cd data
git clone https://github.com/InternLM/xtuner.git
mv xtuner/README_zh-CN.md ./
```

![1733200133646](image/readme/1733200133646.png)

æˆ‘è¿™å„¿ä¸ºäº†å¹²å‡€ç‚¹ï¼Œå°±æŠŠxtuneré‡Œé¢çš„å…¶ä»–ä¸œè¥¿éƒ½åˆ é™¤äº†ï¼Œå°±ç•™ä¸‹äº†ä¸­æ–‡çš„è§£é‡Šï¼Œè¿™ä¸ªä¹Ÿæ˜¯æˆ‘ä»¬ç°åœ¨éœ€è¦çš„ä¸œè¥¿ã€‚

![1733200164258](image/readme/1733200164258.png)

å’Œä¹‹å‰é‚£ä¸ªä¸€æ ·çš„ï¼Œæˆ‘ä»¬å°±æ˜¯å°†dataç›®å½•ä¸‹çš„æ–‡æœ¬æ–‡ä»¶ä½œä¸ºäº†æˆ‘ä»¬çš„çŸ¥è¯†åº“ï¼Œç„¶åè°ƒç”¨çš„æ—¶å€™åŠ è½½ä¸€ä¸‹å°±å¯ä»¥ã€‚

è¿™è¾¹å†åŠ ä¸€ä¸ª `app.py`åé¢æ”¾åˆ°HFçš„æ—¶å€™å°±ä¼šè¿è¡Œå®ƒã€‚

```Python
import os
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.legacy.callbacks import CallbackManager
from llama_index.llms.openai_like import OpenAILike

callback_manager = CallbackManager()

api_base_url =  "https://internlm-chat.intern-ai.org.cn/puyu/api/v1/"
model = "internlm2.5-latest"

# é€šè¿‡Spacesçš„secretä¼ å…¥
api_key = os.environ.get('API_KEY')

llm =OpenAILike(model=model, api_base=api_base_url, api_key=api_key, is_chat_model=True,callback_manager=callback_manager)

st.set_page_config(page_title="llama_index_demo", page_icon="ğŸ¦œğŸ”—")
st.title("llama_index_demo")

os.system('git lfs install')
os.system('git clone https://www.modelscope.cn/Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2.git')

# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def init_models():
    embed_model = HuggingFaceEmbedding(
        model_name="./paraphrase-multilingual-MiniLM-L12-v2"
    )
    Settings.embed_model = embed_model

    #ç”¨åˆå§‹åŒ–llm
    Settings.llm = llm

    documents = SimpleDirectoryReader("./data").load_data()
    index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine()

    return query_engine

# æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–æ¨¡å‹
if 'query_engine' not in st.session_state:
    st.session_state['query_engine'] = init_models()

def greet2(question):
    response = st.session_state['query_engine'].query(question)
    return response

  
# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]  

    # Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]

st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# Function for generating LLaMA2 response
def generate_llama_index_response(prompt_input):
    return greet2(prompt_input)

# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Gegenerate_llama_index_response last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama_index_response(prompt)
            placeholder = st.empty()
            placeholder.markdown(response)
    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)
```

è¿˜è¦åœ¨æˆ‘ä»¬çš„è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹é¢åŠ ä¸€ä¸ª `requirements.txt`å› ä¸º `app.py`é‡Œé¢ç”¨äº†ä¸€äº›ç¬¬ä¸‰æ–¹åº“ï¼Œæˆ‘ä»¬éœ€è¦åŠ ä¸Šå»ï¼Œè®© `Space`è‡ªå·±è¯†åˆ«åˆ°åè‡ªå·±å®‰è£…ã€‚

```
llama-index==0.11.20
llama-index-llms-replicate==0.3.0
llama-index-llms-openai-like==0.2.0
llama-index-embeddings-huggingface==0.3.1
llama-index-embeddings-instructor==0.2.1
sentence-transformers==2.7.0
```

åŸºæœ¬å®Œæˆï¼Œæ¥ä¸‹æ¥å°±æ˜¯å°†æˆ‘ä»¬çš„è¿™ä¸ª `codespace`çš„ä¸œè¥¿æ”¾åˆ°æˆ‘ä»¬HFä¸Šé¢ã€‚

ç›´æ¥ç”¨gitæäº¤å°±å¯ä»¥äº†ã€‚

```Plain
git add .
git commit -m "add app & data"
git push
```

ä¸è¿‡å‡å¦‚ä¹‹å‰æ²¡æœ‰ç”¨è¿‡HFæ˜¯ç°åœ¨ä¸´æ—¶æèµ·çš„è¯ï¼Œè¿™é‡Œä¼šå› ä¸ºæ²¡æœ‰ä½ çš„tokenå¯¼è‡´æ— æ³•ä¸Šä¼ ï¼Œéœ€è¦åœ¨HFç¤¾åŒºè‡ªå·±åŠ ä¸€ä¸ªwriteçš„token

![1733200466845](image/readme/1733200466845.png)

è·å–åˆ°è¿™ä¸ªtokenä»¥åè‡ªå·±ä¿å­˜å¥½ï¼Œç„¶åæˆ‘ä»¬åœ¨ `codespace`ä¸­è¾“å…¥

    ``git remote set-url origin https://<ç”¨æˆ·å>:<ä½ çš„token>@<ä½ çš„é‚£ä¸ªSpaceçš„åœ°å€ï¼Œè®°å¾—æŠŠhttps://å»æ‰>``

ä»¥æˆ‘çš„ä¸ºä¾‹ï¼š

![1733200638197](image/readme/1733200638197.png)

ç„¶åå†è¯•è¯• ``git push``å°±å¯ä»¥äº†

è¿™è¾¹çš„å·¥ä½œéƒ½å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ç»§ç»­å›åˆ°HFï¼Œå› ä¸º `app.py`é‡Œé¢å¸¦äº†ä¸€ä¸ªä¸œè¥¿å°±æ˜¯ `api_key`æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨HFçš„ `secret`ä¼ å…¥ï¼Œå› æ­¤è¿™é‡Œé¢éœ€è¦è®¾ç½®ä¸€ä¸‹ã€‚

![1733200848335](image/readme/1733200848335.png)

è¾“å…¥å®Œæˆåå°±æ˜¯æˆ‘è¿™æ ·çš„ã€‚

![1733200992856](image/readme/1733200992856.png)

æœ€åå°±æ˜¯è¿™æ ·ã€‚

![1733201184996](image/readme/1733201184996.png)

![1733201157769](image/readme/1733201157769.png)

![1733201217523](image/readme/1733201217523.png)
